
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split
from functions import read_database, label_encoding, process_text
from tensorflow.keras.utils import to_categorical

# 1. Load and preprocess data
sentences_raw, labels_raw, all_intents = read_database('../../db.sqlite3')
for i in range(len(sentences_raw)):
    sentences_raw[i] = process_text(sentences_raw[i])

# Split the data into train and test sets
sentences_train, sentences_test, labels_train, labels_test = train_test_split(sentences_raw, labels_raw, test_size=0.3)

# 2. Tokenize the data
tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
tokenizer.fit_on_texts(sentences_train)
word_index = tokenizer.word_index
train_sequences = tokenizer.texts_to_sequences(sentences_train)
test_sequences = tokenizer.texts_to_sequences(sentences_test)

# Padding sequences
train_padded = pad_sequences(train_sequences, maxlen=50, padding='post', truncating='post')
test_padded = pad_sequences(test_sequences, maxlen=50, padding='post', truncating='post')

# Convert labels to one-hot vectors
y_train = label_encoding(labels_train)
y_test = label_encoding(labels_test)
train_labels = to_categorical(y_train)
test_labels = to_categorical(y_test)

# 3. Create a CNN model
model = Sequential()
model.add(Embedding(5000, 64, input_length=50))
model.add(Conv1D(128, 5, activation='relu'))
model.add(GlobalMaxPooling1D())
model.add(Dense(32, activation='relu'))
model.add(Dense(len(all_intents), activation='softmax'))

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# 4. Train the model
model.fit(train_padded, train_labels, epochs=10, validation_data=(test_padded, test_labels))

# 5. Save the trained model
model.save("./data/cnn_model")


{
    "source": "repos/chatbot/chatbot_project/train.py"
}

